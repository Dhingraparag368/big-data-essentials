{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive Assignment 2. DML: Find Most Popular Tags\n",
    "Compare most popular tags in year 2016 with tags in 2009. Tag popularity is the number of questions (post_type_id=1) with this tag. Output top 10 tags in format:\n",
    "\n",
    "\n",
    "<code>tag <tab> rank in 2016 <tab> rank in 2009 <tab> tag popularity in 2016 <tab> tag popularity in 2009</code>\n",
    "\n",
    "Example:\n",
    "\n",
    "<code>php 5 3 1234 5678</code>\n",
    "\n",
    "For 'rank' in each year use rank() function. Order by 'rank in 2016'.\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "<code>...\n",
    "android 3   52  1809    25\n",
    "php 4   3   1673    215\n",
    "python  5   11  1585    108\n",
    "c#  6   1   1519    423\n",
    "...</code>\n",
    "\n",
    "Please use the tables 'posts' and 'users' from the database 'stackoverflow_'. 'posts' is partitioned by year and by month. For more details see \"Hive assignment. Intro and instructions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing describe.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile describe.hql\n",
    "\n",
    "USE stackoverflow_;\n",
    "\n",
    "describe formatted posts;\n",
    "select * from posts limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.159 seconds\n",
      "OK\n",
      "# col_name            \tdata_type           \tcomment             \n",
      "\t \t \n",
      "id                  \tint                 \t                    \n",
      "post_type_id        \ttinyint             \t                    \n",
      "date                \tstring              \t                    \n",
      "owner_user_id       \tint                 \t                    \n",
      "parent_id           \tint                 \t                    \n",
      "score               \tint                 \t                    \n",
      "favorite_count      \tint                 \t                    \n",
      "tags                \tarray<string>       \t                    \n",
      "\t \t \n",
      "# Partition Information\t \t \n",
      "# col_name            \tdata_type           \tcomment             \n",
      "\t \t \n",
      "year                \tstring              \t                    \n",
      "month               \tstring              \t                    \n",
      "\t \t \n",
      "# Detailed Table Information\t \t \n",
      "Database:           \tstackoverflow_      \t \n",
      "Owner:              \tjovyan              \t \n",
      "CreateTime:         \tSat Jul 28 17:49:52 UTC 2018\t \n",
      "LastAccessTime:     \tUNKNOWN             \t \n",
      "Protect Mode:       \tNone                \t \n",
      "Retention:          \t0                   \t \n",
      "Location:           \thdfs://localhost:9000/user/jovyan/precreate/posts\t \n",
      "Table Type:         \tMANAGED_TABLE       \t \n",
      "Table Parameters:\t \t \n",
      "\ttransient_lastDdlTime\t1532800192          \n",
      "\t \t \n",
      "# Storage Information\t \t \n",
      "SerDe Library:      \torg.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe\t \n",
      "InputFormat:        \torg.apache.hadoop.hive.ql.io.RCFileInputFormat\t \n",
      "OutputFormat:       \torg.apache.hadoop.hive.ql.io.RCFileOutputFormat\t \n",
      "Compressed:         \tNo                  \t \n",
      "Num Buckets:        \t8                   \t \n",
      "Bucket Columns:     \t[date]              \t \n",
      "Sort Columns:       \t[Order(col:id, order:1)]\t \n",
      "Storage Desc Params:\t \t \n",
      "\tserialization.format\t1                   \n",
      "Time taken: 1.325 seconds, Fetched: 39 row(s)\n",
      "OK\n",
      "45\t2\t2008-08-01T12:56:37.920\t39\t39\t34\tNULL\tNULL\t2008\t2008-08\n",
      "243\t2\t2008-08-01T22:31:36.137\t71\t234\t12\tNULL\tNULL\t2008\t2008-08\n",
      "304\t2\t2008-08-02T01:38:14.077\t91\t109\t2\tNULL\tNULL\t2008\t2008-08\n",
      "629\t2\t2008-08-03T07:28:54.070\t122\t626\t38\tNULL\tNULL\t2008\t2008-08\n",
      "893\t2\t2008-08-03T23:38:05.113\t234\t871\t6\tNULL\tNULL\t2008\t2008-08\n",
      "1394\t2\t2008-08-04T16:38:03.667\t91\t1390\t16\tNULL\tNULL\t2008\t2008-08\n",
      "1686\t2\t2008-08-04T23:08:18.603\t316\t1669\t43\tNULL\tNULL\t2008\t2008-08\n",
      "2069\t2\t2008-08-05T10:22:55.653\t307\t2056\t23\tNULL\tNULL\t2008\t2008-08\n",
      "2712\t2\t2008-08-05T19:09:08.213\t350\t2639\t10\tNULL\tNULL\t2008\t2008-08\n",
      "2799\t2\t2008-08-05T20:26:08.103\t308\t2767\t34\tNULL\tNULL\t2008\t2008-08\n",
      "Time taken: 1.156 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "!hive -f describe.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.hql\n",
    "\n",
    "USE stackoverflow_;\n",
    "\n",
    "select\n",
    "    tag,\n",
    "    rank() over (sort by cnt2016 desc) as rnk2016,\n",
    "    rank() over (sort by cnt2009 desc) as rnk2009,\n",
    "    cnt2016,\n",
    "    cnt2009\n",
    "from (\n",
    "    select\n",
    "        tag,        \n",
    "        sum(if(year='2016', 1, 0)) as cnt2016,\n",
    "        sum(if(year='2009', 1, 0)) as cnt2009\n",
    "    from posts\n",
    "        lateral view explode(tags) lateral_table as tag\n",
    "    where post_type_id=1 and year in ('2016','2009')\n",
    "    group by tag\n",
    ") as t\n",
    "order by rnk2016\n",
    "limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.184 seconds\n",
      "Query ID = jovyan_20190504110000_ab910bbe-40f6-4a79-bd2f-10cd3a1baf06\n",
      "Total jobs = 4\n",
      "Launching Job 1 out of 4\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1556965619923_0007, Tracking URL = http://cbaa1642b25a:8088/proxy/application_1556965619923_0007/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1556965619923_0007\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-05-04 11:00:44,580 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-05-04 11:00:54,433 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.13 sec\n",
      "2019-05-04 11:01:03,224 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 15.45 sec\n",
      "MapReduce Total cumulative CPU time: 15 seconds 450 msec\n",
      "Ended Job = job_1556965619923_0007\n",
      "Launching Job 2 out of 4\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1556965619923_0008, Tracking URL = http://cbaa1642b25a:8088/proxy/application_1556965619923_0008/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1556965619923_0008\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2019-05-04 11:01:21,151 Stage-2 map = 0%,  reduce = 0%\n",
      "2019-05-04 11:01:28,818 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.3 sec\n",
      "2019-05-04 11:01:38,492 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 9.62 sec\n",
      "MapReduce Total cumulative CPU time: 9 seconds 620 msec\n",
      "Ended Job = job_1556965619923_0008\n",
      "Launching Job 3 out of 4\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1556965619923_0009, Tracking URL = http://cbaa1642b25a:8088/proxy/application_1556965619923_0009/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1556965619923_0009\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1\n",
      "2019-05-04 11:01:55,834 Stage-3 map = 0%,  reduce = 0%\n",
      "2019-05-04 11:02:04,384 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 3.87 sec\n",
      "2019-05-04 11:02:13,999 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 9.84 sec\n",
      "MapReduce Total cumulative CPU time: 9 seconds 840 msec\n",
      "Ended Job = job_1556965619923_0009\n",
      "Launching Job 4 out of 4\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1556965619923_0010, Tracking URL = http://cbaa1642b25a:8088/proxy/application_1556965619923_0010/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1556965619923_0010\n",
      "Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1\n",
      "2019-05-04 11:02:31,477 Stage-4 map = 0%,  reduce = 0%\n",
      "2019-05-04 11:02:39,003 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 3.97 sec\n",
      "2019-05-04 11:02:48,595 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 7.79 sec\n",
      "MapReduce Total cumulative CPU time: 7 seconds 790 msec\n",
      "Ended Job = job_1556965619923_0010\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 15.45 sec   HDFS Read: 970676 HDFS Write: 316407 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 9.62 sec   HDFS Read: 322315 HDFS Write: 347254 SUCCESS\n",
      "Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 9.84 sec   HDFS Read: 353612 HDFS Write: 378100 SUCCESS\n",
      "Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 7.79 sec   HDFS Read: 383259 HDFS Write: 188 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 42 seconds 700 msec\n",
      "OK\n",
      "javascript\t1\t5\t2771\t192\n",
      "java\t2\t2\t2033\t243\n",
      "android\t3\t52\t1809\t25\n",
      "php\t4\t3\t1673\t215\n",
      "python\t5\t11\t1585\t108\n",
      "c#\t6\t1\t1519\t423\n",
      "html\t7\t14\t1212\t84\n",
      "jquery\t8\t8\t1167\t141\n",
      "ios\t9\t186\t914\t7\n",
      "css\t10\t20\t801\t59\n",
      "Time taken: 142.225 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "!hive -f task.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
