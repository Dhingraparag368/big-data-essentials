{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Applications: TF-IDF\n",
    "In this task Hadoop Streaming is used to process Wikipedia articles dump (/data/wiki/en_articles_part).\n",
    "\n",
    "The purpose of this task is to calculate <code>tf*idf</code> for each pair (word, article) from the Wikipedia dump. Apply the stop words filter to speed up calculations. Term frequency (tf) is a function depending on a term (word) and a document (article):\n",
    "\n",
    "<code>tf(term, doc_id) = Nt/N</code>,\n",
    "\n",
    "where Nt - quantity of particular term in the document, N - the total number of terms in the document (without stop words)\n",
    "\n",
    "Inverse document frequency (idf) is a function depends on a term:\n",
    "\n",
    "<code>idf(term) = 1/log(1 + Dt)</code>,\n",
    "\n",
    "where Dt - number of documents in the dataset with the particular term.\n",
    "\n",
    "You can find more information here: https://en.wikipedia.xn--org/wiki/Tfidf-q82h but use just the formulas mentioned above.\n",
    "\n",
    "Dataset location: /data/wiki/en_articles_part\n",
    "\n",
    "Stop words list is in ‘/datasets/stop_words_en.txt’ file.\n",
    "\n",
    "Format: article_id <i>tab</i> article_text\n",
    "\n",
    "To parse the articles don’t forget about Unicode (even though this is an English Wikipedia dump, there are many characters from other languages), remove punctuation marks and transform words to lowercase to get the correct quantities. To cope with Unicode we recommend to use the following tokenizer:\n",
    "\n",
    "Output: <code>tf*idf</code> for term=’labor’ and article_id=12\n",
    "\n",
    "The result on the sample dataset:\n",
    "\n",
    "<code>0.000351</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.dat\n",
    "\n",
    "1\tIn this task Hadoop Streaming is used to process Wikipedia articles dump calculations.\n",
    "2\tThe purpose of this task is to calculate tf*idf for each pair (word, article) from the Wikipedia dump calculations.\n",
    "3\tApply the stop words filter to speed up calculations calculations calculations calculations calculations. \n",
    "4\tTerm frequency (tf) is a function depending on a term (word) and a document (article)\n",
    "5\tTo parse the articles don’t forget about Unicode (even though this is an English Wikipedia dump, there are many characters from other languages), remove punctuation marks and transform words to lowercase to get the correct quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "with open('/datasets/stop_words_en.txt', \"r\") as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "    \n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = unicode(line.strip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    text= re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "    text = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)   \n",
    "    \n",
    "    tf = dict()\n",
    "    for w in text:\n",
    "        w = w.lower()\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "            \n",
    "        if w in tf:\n",
    "            tf[w] += 1\n",
    "        else:\n",
    "            tf[w] = 1\n",
    "            \n",
    "    total = len(tf)    \n",
    "    for term, count in tf.iteritems():\n",
    "        print \"%s\\t%s\\t%f\" % (term, article_id, float(count)/total) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles\t1\t0.111111\r\n",
      "task\t1\t0.111111\r\n",
      "dump\t1\t0.111111\r\n",
      "process\t1\t0.111111\r\n",
      "wikipedia\t1\t0.111111\r\n",
      "hadoop\t1\t0.111111\r\n",
      "streaming\t1\t0.111111\r\n",
      "used\t1\t0.111111\r\n",
      "calculations\t1\t0.111111\r\n",
      "tf*idf\t2\t0.100000\r\n"
     ]
    }
   ],
   "source": [
    "cat test.dat | python2 ./mapper1.py | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "current_key = None\n",
    "cache = dict() # TODO try to use join to avoid in-memory collection\n",
    "\n",
    "def process_record(term, cache):\n",
    "    idf = 1/log(1+len(cache))\n",
    "    for article_id, tf in cache.iteritems():\n",
    "        print \"%s\\t%s\\t%f\" % (term, article_id, tf*idf)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        term, article_id, tf = line.strip().split('\\t', 2)\n",
    "        tf = float(tf)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "        \n",
    "    if current_key != term:\n",
    "        if current_key:            \n",
    "            process_record(current_key, cache)\n",
    "        current_key = term\n",
    "        cache.clear()\n",
    "    cache[article_id] = tf # TODO use list\n",
    "    \n",
    "if current_key:\n",
    "    process_record(current_key, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform\t5\t0.080150\r\n",
      "unicode\t5\t0.080150\r\n",
      "used\t1\t0.160299\r\n",
      "wikipedia\t1\t0.080150\r\n",
      "wikipedia\t2\t0.072135\r\n",
      "wikipedia\t5\t0.040075\r\n",
      "word\t2\t0.091024\r\n",
      "word\t4\t0.113780\r\n",
      "words\t3\t0.151707\r\n",
      "words\t5\t0.050569\r\n"
     ]
    }
   ],
   "source": [
    "cat test.dat | python2 ./mapper1.py | sort | python2 ./reducer1.py | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
