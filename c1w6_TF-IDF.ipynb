{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Applications: TF-IDF\n",
    "In this task Hadoop Streaming is used to process Wikipedia articles dump (/data/wiki/en_articles_part).\n",
    "\n",
    "The purpose of this task is to calculate <code>tf*idf</code> for each pair (word, article) from the Wikipedia dump. Apply the stop words filter to speed up calculations. Term frequency (tf) is a function depending on a term (word) and a document (article):\n",
    "\n",
    "<code>tf(term, doc_id) = Nt/N</code>,\n",
    "\n",
    "where Nt - quantity of particular term in the document, N - the total number of terms in the document (without stop words)\n",
    "\n",
    "Inverse document frequency (idf) is a function depends on a term:\n",
    "\n",
    "<code>idf(term) = 1/log(1 + Dt)</code>,\n",
    "\n",
    "where Dt - number of documents in the dataset with the particular term.\n",
    "\n",
    "You can find more information here: https://en.wikipedia.xn--org/wiki/Tfidf-q82h but use just the formulas mentioned above.\n",
    "\n",
    "Dataset location: /data/wiki/en_articles_part\n",
    "\n",
    "Stop words list is in ‘/datasets/stop_words_en.txt’ file.\n",
    "\n",
    "Format: article_id <i>tab</i> article_text\n",
    "\n",
    "To parse the articles don’t forget about Unicode (even though this is an English Wikipedia dump, there are many characters from other languages), remove punctuation marks and transform words to lowercase to get the correct quantities. To cope with Unicode we recommend to use the following tokenizer:\n",
    "\n",
    "Output: <code>tf*idf</code> for term=’labor’ and article_id=12\n",
    "\n",
    "The result on the sample dataset:\n",
    "\n",
    "<code>0.000351</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "with open('/datasets/stop_words_en.txt', \"r\") as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "    \n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        doc, text = unicode(line.strip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    text= re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)   \n",
    "    words = [i.lower() for i in words if i.lower() not in stop_words]\n",
    "    \n",
    "    tf = dict()\n",
    "    for w in words:\n",
    "        if w in tf:\n",
    "            tf[w] += 1\n",
    "        else:\n",
    "            tf[w] = 1\n",
    "            \n",
    "    cnt = len(tf)    \n",
    "    for k,v in tf.iteritems():\n",
    "        print \"%s_%s\\t%f\" % (k, doc, float(v)/cnt) # term_doc <tab> tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.dat\n",
    "\n",
    "1\tIn this task Hadoop Streaming is used to process Wikipedia articles dump calculations.\n",
    "2\tThe purpose of this task is to calculate tf*idf for each pair (word, article) from the Wikipedia dump calculations.\n",
    "3\tApply the stop words filter to speed up calculations calculations calculations calculations calculations. \n",
    "4\tTerm frequency (tf) is a function depending on a term (word) and a document (article)\n",
    "5\tTo parse the articles don’t forget about Unicode (even though this is an English Wikipedia dump, there are many characters from other languages), remove punctuation marks and transform words to lowercase to get the correct quantities.\n",
    "42\tTo cope with Unicode we recommend to use the following tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles_1\t0.111111\r\n",
      "task_1\t0.111111\r\n",
      "dump_1\t0.111111\r\n",
      "process_1\t0.111111\r\n",
      "wikipedia_1\t0.111111\r\n",
      "hadoop_1\t0.111111\r\n",
      "streaming_1\t0.111111\r\n",
      "used_1\t0.111111\r\n",
      "calculations_1\t0.111111\r\n",
      "tf*idf_2\t0.100000\r\n"
     ]
    }
   ],
   "source": [
    "cat test.dat | python2 ./mapper.py | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        key, tf = line.strip().split('\\t', 1)        \n",
    "        term, doc = key.split('_', 1)       \n",
    "        doc = int(doc)\n",
    "        tf = float(tf)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    \n",
    "    # TODO: here\n",
    "    if current_key != permutation:        \n",
    "        if current_key and len(words) > 1:\n",
    "            print \"%d\\t%d\\t%s\" % (key_sum, len(words), ','.join(sorted(words)))\n",
    "        current_key = permutation\n",
    "        key_sum = 0\n",
    "        words = set()\n",
    "    key_sum += count\n",
    "    words.add(word)\n",
    "    \n",
    "if current_key and len(words) > 1:    \n",
    "    print \"%d\\t%d\\t%s\" % (key_sum, len(words), ','.join(sorted(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
