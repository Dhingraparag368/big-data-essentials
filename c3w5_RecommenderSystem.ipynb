{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering recommender system\n",
    "Dataset description\n",
    "In this assignment you will use Apache Spark to build simple movie recommendation system on big MovieLens 20M dataset (https://grouplens.org/datasets/movielens/). The dataset contains 20 million ratings for 27000 movies given by 138000 users.\n",
    "\n",
    "Dataset format: comma-separated values (https://en.wikipedia.org/wiki/CSV)\n",
    "\n",
    "The first line is a header: userId,movieId,rating,timestamp\n",
    "\n",
    "* userId, movieId are integers representing user and movies identifiers correspondingly\n",
    "* rating is a floating point number from 1.0 to 5.0\n",
    "* timestamp is an integer but it wonâ€™t be used in the assignment\n",
    "\n",
    "The dataset is located at <b>/data/movielens</b>.\n",
    "\n",
    "The metric for this assignment is RMSE. You have to achieve score lower than 0.9 on the test dataset in order to get the full score for the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', True) \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .format('csv') \\\n",
    "    .load('/data/movielens/ratings_100k.csv')\n",
    "\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into three folds: on each iteration one fold will be used for testing and the other two folds will be used for training. Wrap necessary values with Rating class.\n",
    "\n",
    "Now it is time to choose training parameters. It is recommended to set the rank equal to 20, regularization term lambda equal to 0.01 and the number of iterations equal to 5. Feel free to experiment and choose your own parameters to see how they influence the final score.\n",
    "\n",
    "Train the explicit ALS model on two of three folds and evaluate its performance on remaining test fold. Performance evaluation should be done using the classic RMSE metric.\n",
    "\n",
    "Average RMSE scores on all the three folds and output the result to stdout. You could refer to publicly available benchmarks (e.g. http://mymedialite.net/examples/datasets.html) to find out what score to expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.mllib RDD API\n",
    "https://spark.apache.org/docs/2.2.0/mllib-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from math import sqrt\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "from pyspark.mllib.recommendation import ALS as RDD_ALS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def evaluate_model(train, test):\n",
    "    model = RDD_ALS.train(train, rank=20, iterations=5, lambda_=0.01, seed=13)\n",
    "    pred = model.predictAll(test.map(lambda x: (x[0],x[1])))\n",
    "    \n",
    "    to_key = lambda x: ((x[0],x[1]),x[2])\n",
    "    mse = test.map(to_key) \\\n",
    "        .join(pred.map(to_key)) \\\n",
    "        .values() \\\n",
    "        .map(lambda x: (x[0]-x[1])**2) \\\n",
    "        .mean()\n",
    "        \n",
    "    return sqrt(mse)\n",
    "\n",
    "X = df.rdd.map(lambda x: Rating(x[0],x[1],x[2])).randomSplit([1,1,1], seed=13)\n",
    "X = [rdd.cache() for rdd in X]\n",
    "\n",
    "scores = [evaluate_model(X[0]+X[1], X[2]), \n",
    "          evaluate_model(X[1]+X[2], X[0]), \n",
    "          evaluate_model(X[0]+X[2], X[1])]\n",
    "\n",
    "print(sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.ml DataFrame API\n",
    "https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropNaRegressionEvaluator(RegressionEvaluator):\n",
    "    def evaluate(self, df):\n",
    "        return RegressionEvaluator.evaluate(self, df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.28693504266\n"
     ]
    }
   ],
   "source": [
    "als = ALS(userCol='userId', itemCol='movieId', ratingCol='rating', seed=13)\n",
    "\n",
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [20]) \\\n",
    "    .addGrid(als.maxIter, [5]) \\\n",
    "    .addGrid(als.regParam, [0.01]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = DropNaRegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3, seed=13) \\\n",
    "    .fit(df)\n",
    "\n",
    "print(cv.avgMetrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
