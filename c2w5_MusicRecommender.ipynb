{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "There are two data sources for this assignment. They are DataFrames in parquet format.\n",
    "\n",
    "The first dataset captures the user’s playing history.\n",
    "\n",
    "Location - /data/sample264\n",
    "\n",
    "Fields: trackId, userId, timestamp, artistId\n",
    "\n",
    "* trackId - id of the track\n",
    "* userId - id of the user\n",
    "* artistId - id of the artist\n",
    "* timestamp - timestamp of the moment the user starts listening to a track\n",
    "\n",
    "The second is the meta data for track or artist.\n",
    "\n",
    "Location - /data/meta\n",
    "\n",
    "Fields: type, Name, Artist, Id\n",
    "\n",
    "* Type could be “track” or “artist”\n",
    "* Name is the title of the track, if the type == “track” and the name of the musician or group, if the type == “artist”.\n",
    "* Artist states for the creator of the track in case the type == “track” and for the name of the musician or group in case the type == “artist”.\n",
    "* Id - id of the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparkSession.read.parquet(\"/data/sample264\")\n",
    "meta = sparkSession.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph based Music Recommender. Task 1\n",
    "Build the edges of the type “track-track”. To do it you will need to count the collaborative similarity between all the tracks: if a user has started listening to track B within 7 minutes after starting track A, then you should add 1 to the weight of the edge from vertex A to vertex B (initial weight is equal to 0).\n",
    "\n",
    "Example:\n",
    "\n",
    "<code>\n",
    "userId artistId trackId timestamp\n",
    "7        12        1          1534574189\n",
    "7        13        4          1534574289 \n",
    "5        12        1          1534574389 \n",
    "5        13        4          1534594189 \n",
    "6        12        1          1534574489 \n",
    "6        13        4          1534574689\n",
    "</code>\n",
    "\n",
    "The track 1 is similar to the track 4 with the weight 2 (before normalization): the user 7 and the user 6 listened these 2 tracks together in the 7 minutes long window:\n",
    "\n",
    "<code>\n",
    "userId 7: 1534574289 - 1534574189 = 100 seconds = 1 min 40 seconds < 7 minutes\n",
    "userId 6: 1534574689 - 1534574489 = 200 seconds = 3 min 20 seconds < 7 minutes\n",
    "</code>\n",
    "\n",
    "Note that the track 4 is similar to the track 1 with the same weight 2.\n",
    "\n",
    "Tip: consider joining the graph to itself with the UserId and remove pairs with the same tracks.For each track choose top 50 tracks ordered by weight similar to it and normalize weights of its edges (divide the weight of each edge on a sum of weights of all edges). Use rank() to choose top 40 tracks as is done in the demo.\n",
    "\n",
    "Sort the resulting Data Frame in the descending order by the column norm_weight, and then in the ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "Output example:\n",
    "\n",
    "<code>\n",
    "54719\t\t767867\n",
    "54719\t\t767866\n",
    "50787\t\t327676\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, key1, key2, field, n):    \n",
    "    window = Window.partitionBy(key1).orderBy(f.col(field).desc())\n",
    "    topsDF = df.withColumn(\"row_number\", f.row_number().over(window)) \\\n",
    "        .filter(f.col(\"row_number\") <= n) \\\n",
    "        .drop(f.col(\"row_number\"))\n",
    "    tmpDF = topsDF.groupBy(f.col(key1)).agg(f.col(key1), f.sum(f.col(field)).alias(\"sum_\" + field))\n",
    "    normalizedDF = topsDF.join(tmpDF, key1, \"inner\") \\\n",
    "        .withColumn(\"norm_\" + field, f.col(field) / f.col(\"sum_\" + field))\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "w = Window.partitionBy('userId').orderBy('timestamp')\n",
    "\n",
    "tracks = data \\\n",
    "    .select(\n",
    "        'userId',\n",
    "        'trackId',\n",
    "        f.lag('timestamp').over(w).alias('timestamp_lag'),\n",
    "        f.lag('trackId').over(w).alias('trackId_lag')) \\\n",
    "    .filter(f.col('timestamp') - f.col('timestamp_lag') < 7*60) \\\n",
    "    .select(f.col('trackId').alias('id1'), f.col('trackId_lag').alias('id2')) \\\n",
    "    .groupBy('id1', 'id2').agg(f.count(\"*\").alias('cnt1')) \\\n",
    "    .cache()\n",
    "    \n",
    "reverted = tracks.select(\n",
    "    f.col('id2').alias('id1'), \n",
    "    f.col('id1').alias('id2'), \n",
    "    f.col('cnt1').alias('cnt2'))\n",
    "\n",
    "df = tracks.join(reverted, on=['id1','id2'], how='left') \\\n",
    "    .select(\n",
    "        'id1', \n",
    "        'id2', \n",
    "        (f.col('cnt1') + f.when(f.isnull('cnt2'), 0).otherwise(f.col('cnt2'))).alias('count'))\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798256\t923706\n",
      "798319\t837992\n",
      "798322\t876562\n",
      "798331\t827364\n",
      "798335\t840741\n",
      "798374\t816874\n",
      "798375\t810685\n",
      "798379\t812055\n",
      "798380\t840113\n",
      "798396\t817687\n",
      "798398\t926302\n",
      "798405\t867217\n",
      "798443\t905923\n",
      "798457\t918918\n",
      "798460\t891840\n",
      "798461\t940379\n",
      "798470\t840814\n",
      "798474\t963162\n",
      "798477\t883244\n",
      "798485\t955521\n",
      "798505\t905671\n",
      "798545\t949238\n",
      "798550\t936295\n",
      "798626\t845438\n",
      "798691\t818279\n",
      "798692\t898823\n",
      "798702\t811440\n",
      "798704\t937570\n",
      "798725\t933147\n",
      "798738\t894170\n",
      "798745\t799665\n",
      "798782\t956938\n",
      "798801\t950802\n",
      "798820\t890393\n",
      "798833\t916319\n",
      "798865\t962662\n",
      "798931\t893574\n",
      "798946\t946408\n",
      "799012\t809997\n",
      "799024\t935246\n"
     ]
    }
   ],
   "source": [
    "def task1():\n",
    "    A = data.select('userId', f.col('trackId').alias('track1'), f.col('timestamp').alias('ts1'))\n",
    "    B = data.select('userId',f.col('trackId').alias('track2'), f.col('timestamp').alias('ts2'))\n",
    "\n",
    "    AB = A.join(B, on=['userId']) \\\n",
    "        .filter((A['track1'] != B['track2']) & (f.abs(A['ts1']-B['ts2']) <= 7*60)) \\\n",
    "        .groupBy('track1', 'track2') \\\n",
    "        .agg(f.count(\"*\").alias('count')) \\\n",
    "        .cache()\n",
    "\n",
    "    df = normalize(AB, 'track1', 'track2', 'count', 1000) \\\n",
    "        .orderBy(f.desc('norm_count'), 'track1', 'track2') \\\n",
    "        .select('track1', 'track2') \\\n",
    "        .limit(40)\n",
    "\n",
    "    for t1, t2 in df.collect():\n",
    "        print(\"{}\\t{}\".format(t1,t2))\n",
    "\n",
    "#task1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph based Music Recommender. Task 2\n",
    "Build the edges of the type “user-track”. Take the amount of times the track was listened by the user as the weight of the edge from the user’s vertex to the track’s vertex.\n",
    "\n",
    "Tip: group the dataframe by columns userId and trackId and use function “count” of DF API.\n",
    "\n",
    "For each user take top-1000 and normalize them.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column norm_weight, and then in ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "<code>\n",
    "...\n",
    "195 946408\n",
    "215 860111\n",
    "235 897176\n",
    "300 857973\n",
    "321 915545\n",
    "...\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\t965774\n",
      "116\t867268\n",
      "128\t852564\n",
      "131\t880170\n",
      "195\t946408\n",
      "215\t860111\n",
      "235\t897176\n",
      "300\t857973\n",
      "321\t915545\n",
      "328\t943482\n",
      "333\t818202\n",
      "346\t864911\n",
      "356\t961308\n",
      "428\t943572\n",
      "431\t902497\n",
      "445\t831381\n",
      "488\t841340\n",
      "542\t815388\n",
      "617\t946395\n",
      "649\t901672\n",
      "658\t937522\n",
      "662\t881433\n",
      "698\t935934\n",
      "708\t952432\n",
      "746\t879259\n",
      "747\t879259\n",
      "776\t946408\n",
      "784\t806468\n",
      "806\t866581\n",
      "811\t948017\n",
      "837\t799685\n",
      "901\t871513\n",
      "923\t879322\n",
      "934\t940714\n",
      "957\t945183\n",
      "989\t878364\n",
      "999\t967768\n",
      "1006\t962774\n",
      "1049\t849484\n",
      "1057\t920458\n"
     ]
    }
   ],
   "source": [
    "def task2():\n",
    "    X = data.select('userId', 'trackId').groupBy('userId', 'trackId').agg(f.count(\"*\").alias('count')).cache()\n",
    "\n",
    "    df = normalize(X, 'userId', 'trackId', 'count', 1000) \\\n",
    "        .orderBy(f.desc('norm_count'), 'userId', 'trackId') \\\n",
    "        .select('userId', 'trackId') \\\n",
    "        .limit(40)\n",
    "\n",
    "    for t1, t2 in df.collect():\n",
    "        print(\"{}\\t{}\".format(t1,t2))\n",
    "        \n",
    "#task2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph based Music Recommender. Task 3\n",
    "Build the edges of the type “user-artist”. Take the amount of times the user has listened to the artist’s tracks as the weight of the edge from the user’s vertex to the artist’s vertex.\n",
    "\n",
    "Tip: group the dataframe by the columns userId and trackId and use the function “count” of DF API. For each user take top-100 artists and normalize weights.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column norm_weight, and then in ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "<code>\n",
    "...\n",
    "131 983068\n",
    "195 997265\n",
    "215 991696\n",
    "235 990642\n",
    "288 1000564\n",
    "...\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\t993426\n",
      "116\t974937\n",
      "128\t1003021\n",
      "131\t983068\n",
      "195\t997265\n",
      "215\t991696\n",
      "235\t990642\n",
      "288\t1000564\n",
      "300\t1003362\n",
      "321\t986172\n",
      "328\t967986\n",
      "333\t1000416\n",
      "346\t982037\n",
      "356\t974846\n",
      "374\t1003167\n",
      "428\t993161\n",
      "431\t969340\n",
      "445\t970387\n",
      "488\t970525\n",
      "542\t969751\n",
      "612\t987351\n",
      "617\t970240\n",
      "649\t973851\n",
      "658\t973232\n",
      "662\t975279\n",
      "698\t995788\n",
      "708\t968848\n",
      "746\t972032\n",
      "747\t972032\n",
      "776\t997265\n",
      "784\t969853\n",
      "806\t995126\n",
      "811\t996436\n",
      "837\t989262\n",
      "901\t988199\n",
      "923\t977066\n",
      "934\t990860\n",
      "957\t991171\n",
      "989\t975339\n",
      "999\t968823\n"
     ]
    }
   ],
   "source": [
    "def task3():\n",
    "    X = data.select('userId', 'artistId').groupBy('userId', 'artistId').agg(f.count(\"*\").alias('count')).cache()\n",
    "\n",
    "    df = normalize(X, 'userId', 'artistId', 'count', 1000) \\\n",
    "        .orderBy(f.desc('norm_count'), 'userId', 'artistId') \\\n",
    "        .select('userId', 'artistId') \\\n",
    "        .limit(40)\n",
    "\n",
    "    for t1, t2 in df.collect():\n",
    "        print(\"{}\\t{}\".format(t1,t2))\n",
    "\n",
    "#task3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph based Music Recommender. Task 4\n",
    "Build the edges of the type “artist-track”. Take the amount of times the track HAS BEEN listened by all users as the weight of the edge from the artist’s vertex to the track’s vertex.\n",
    "\n",
    "Tip: group the dataframe by the columns “artistId” and “trackId” and use the function “count” of DF API. For each artist take top-100 tracks and normalize weights.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column norm_weight, and then in ascending order this time first by “id1”, then by “id2”. Take top 40 rows, select only the columns “id1”, “id2”, and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "<code>\n",
    "...\n",
    "968017 859321\n",
    "968022 852786\n",
    "968034 807671\n",
    "968038 964150\n",
    "968042 835935\n",
    "... \n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967993\t869415\n",
      "967998\t947428\n",
      "968004\t927380\n",
      "968017\t859321\n",
      "968022\t852786\n",
      "968034\t807671\n",
      "968038\t964150\n",
      "968042\t835935\n",
      "968043\t913568\n",
      "968046\t935077\n",
      "968047\t806127\n",
      "968065\t907906\n",
      "968073\t964586\n",
      "968086\t813446\n",
      "968092\t837129\n",
      "968118\t914441\n",
      "968125\t821410\n",
      "968140\t953008\n",
      "968148\t877445\n",
      "968161\t809793\n",
      "968163\t803065\n",
      "968168\t876119\n",
      "968189\t858639\n",
      "968221\t896937\n",
      "968224\t892880\n",
      "968232\t825536\n",
      "968237\t932845\n",
      "968238\t939177\n",
      "968241\t879045\n",
      "968242\t911250\n",
      "968248\t953554\n",
      "968255\t808494\n",
      "968259\t880230\n",
      "968265\t950148\n",
      "968266\t824437\n",
      "968269\t913243\n",
      "968272\t816049\n",
      "968278\t946743\n",
      "968285\t847460\n",
      "968286\t940006\n"
     ]
    }
   ],
   "source": [
    "def task4():\n",
    "    X = data.select('artistId', 'trackId').groupBy('artistId', 'trackId').agg(f.count(\"*\").alias('count')).cache()\n",
    "\n",
    "    df = normalize(X, 'artistId', 'trackId', 'count', 1000) \\\n",
    "        .orderBy(f.desc('norm_count'), 'artistId', 'trackId') \\\n",
    "        .select('artistId', 'trackId') \\\n",
    "        .limit(40)\n",
    "\n",
    "    for t1, t2 in df.collect():\n",
    "        print(\"{}\\t{}\".format(t1,t2))\n",
    "\n",
    "#task4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph based Music Recommender. Task 5\n",
    "For the user with Id 776748 find all the tracks and artists connected to him. Use original dataframe not a normalized one. Sort founded items first by artist then by name in ascending order, leave only columns ”Artist” and “Name” and print top-40.\n",
    "\n",
    "Each output line can take one of the following forms:\n",
    "\n",
    "<code>\n",
    "Artist: {artist-name} {track-name}\n",
    "Artist: {artist-name} Artist: {artist-name}\n",
    "</code>\n",
    "\n",
    "These two forms help distinguish “user-track” suggestions (as shown in 1) from “user-artist” suggestions (as shown in 2).\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "<code>\n",
    "...\n",
    "Artist: Blur Artist: Blur\n",
    "Artist: Blur Girls and Boys\n",
    "Artist: Clawfinger Artist: Clawfinger\n",
    "Artist: Clawfinger Nothing Going On\n",
    "Artist: Disturbed Artist: Disturbed\n",
    "...\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: 3 Doors Down\tArtist: 3 Doors Down\n",
      "Artist: 3 Doors Down\tKryptonite\n",
      "Artist: 311\tArtist: 311\n",
      "Artist: 311\tBeautiful disaster\n",
      "Artist: Blur\tArtist: Blur\n",
      "Artist: Blur\tGirls and Boys\n",
      "Artist: Clawfinger\tArtist: Clawfinger\n",
      "Artist: Clawfinger\tNothing Going On\n",
      "Artist: Disturbed\tArtist: Disturbed\n",
      "Artist: Disturbed\tThe Vengeful One\n",
      "Artist: Gotthard\tArtist: Gotthard\n",
      "Artist: Gotthard\tEagle\n",
      "Artist: Green Day\t21 Guns\n",
      "Artist: Green Day\tArtist: Green Day\n",
      "Artist: Green Day\tKill The DJ\n",
      "Artist: Iggy Pop\tArtist: Iggy Pop\n",
      "Artist: Iggy Pop\tSunday\n",
      "Artist: Korn\tArtist: Korn\n",
      "Artist: Korn\tHere To Stay\n",
      "Artist: Linkin Park\tArtist: Linkin Park\n",
      "Artist: Linkin Park\tIn The End\n",
      "Artist: Linkin Park\tNumb\n",
      "Artist: Lordi\tArtist: Lordi\n",
      "Artist: Lordi\tHard Rock Hallelujah\n",
      "Artist: Nickelback\tArtist: Nickelback\n",
      "Artist: Nickelback\tShe Keeps Me Up\n",
      "Artist: Nomy\tArtist: Nomy\n",
      "Artist: Nomy\tCocaine\n",
      "Artist: Papa Roach\tArtist: Papa Roach\n",
      "Artist: Papa Roach\tGetting Away With Murder\n",
      "Artist: Rise Against\tArtist: Rise Against\n",
      "Artist: Rise Against\tPrayer Of The Refugee\n",
      "Artist: Serj Tankian\tArtist: Serj Tankian\n",
      "Artist: Serj Tankian\tSky is Over\n",
      "Artist: Slipknot\tArtist: Slipknot\n",
      "Artist: Slipknot\tWait And Bleed\n",
      "Artist: The Offspring\tArtist: The Offspring\n",
      "Artist: The Offspring\tCome Out and Play\n",
      "Artist: Thousand Foot Krutch\tArtist: Thousand Foot Krutch\n",
      "Artist: Thousand Foot Krutch\tTake It Out On Me\n"
     ]
    }
   ],
   "source": [
    "def task5():\n",
    "    log = data.filter(f.col('userId') == 776748).cache()\n",
    "    tracks = meta.filter(f.col('type') == u'track').withColumnRenamed('Id', 'trackId')\n",
    "    artists = meta.filter(f.col('type') == u'artist').withColumnRenamed('Id', 'artistId')\n",
    "\n",
    "    A = log.join(tracks, on=['trackId']).select('Artist', 'Name')\n",
    "    B = log.join(artists, on=['artistId']).select('Artist', 'Name')\n",
    "\n",
    "    df = A.union(B) \\\n",
    "        .orderBy('Artist', 'Name') \\\n",
    "        .select('Artist', 'Name') \\\n",
    "        .distinct() \\\n",
    "        .limit(40)\n",
    "\n",
    "    for t1, t2 in df.collect():\n",
    "        print(\"{}\\t{}\".format(t1,t2))\n",
    "        \n",
    "#task5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
