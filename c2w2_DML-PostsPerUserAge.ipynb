{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive Assignment 3. DML: Calculate Amount of Posts per User Age\n",
    "Use Hive to complete the following task. Input tables was described in Hive Task1 and Hive Task2.\n",
    "\n",
    "Calculate number of questions and answers depending on users' age. Use Ð°ge from 'users' table, filter out users if their age is undefined. Output format:\n",
    "\n",
    "<code>age\\tnumber of questions\\tnumber of answers</code>\n",
    "\n",
    "Example:\n",
    "\n",
    "<code>22 12345 5678</code>\n",
    "\n",
    "Output all ages. Order by age, increment.\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "<code>...\n",
    "21  11  24\n",
    "22  6   18\n",
    "23  12  15\n",
    "24  16  27\n",
    "25  20  33\n",
    "...\n",
    "</code>\n",
    "\n",
    "Hint. To simplify your code and reduce the quantity of MapReduce jobs produced by the query, use IF clause.\n",
    "\n",
    "Please use the tables 'posts' and 'users' from the database 'stackoverflow_'. 'posts' is partitioned by year and by month. For more details see \"Hive assignment. Intro and instructions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.hql\n",
    "\n",
    "USE stackoverflow_;\n",
    "\n",
    "select\n",
    "    age,\n",
    "    sum(if(post_type_id=1,1,0)) as questions,\n",
    "    sum(if(post_type_id=2,1,0)) as answers\n",
    "from users\n",
    "join posts on users.id = posts.owner_user_id\n",
    "where users.age is not null\n",
    "group by age\n",
    "order by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.126 seconds\n",
      "Query ID = jovyan_20190505193737_1ab5ffaf-27e5-4ef6-976a-4d6d2236c68b\n",
      "Total jobs = 2\n",
      "Execution log at: /tmp/jovyan/jovyan_20190505193737_1ab5ffaf-27e5-4ef6-976a-4d6d2236c68b.log\n",
      "2019-05-05 07:37:15\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2019-05-05 07:37:17\tDump the side-table for tag: 0 with group count: 5951 into file: file:/tmp/jovyan/909e8e16-4681-4b73-96d2-0f05ffdb42c4/hive_2019-05-05_19-37-06_443_4942906773307164500-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile00--.hashtable\n",
      "2019-05-05 07:37:17\tUploaded 1 File to: file:/tmp/jovyan/909e8e16-4681-4b73-96d2-0f05ffdb42c4/hive_2019-05-05_19-37-06_443_4942906773307164500-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile00--.hashtable (137665 bytes)\n",
      "2019-05-05 07:37:17\tEnd of local task; Time Taken: 2.189 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1557079047813_0005, Tracking URL = http://686b2428451a:8088/proxy/application_1557079047813_0005/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1557079047813_0005\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2019-05-05 19:37:33,494 Stage-2 map = 0%,  reduce = 0%\n",
      "2019-05-05 19:37:43,456 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 9.08 sec\n",
      "2019-05-05 19:37:52,098 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 12.74 sec\n",
      "MapReduce Total cumulative CPU time: 12 seconds 740 msec\n",
      "Ended Job = job_1557079047813_0005\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1557079047813_0006, Tracking URL = http://686b2428451a:8088/proxy/application_1557079047813_0006/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1557079047813_0006\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1\n",
      "2019-05-05 19:38:09,297 Stage-3 map = 0%,  reduce = 0%\n",
      "2019-05-05 19:38:16,818 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 3.08 sec\n",
      "2019-05-05 19:38:25,385 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 6.86 sec\n",
      "MapReduce Total cumulative CPU time: 6 seconds 860 msec\n",
      "Ended Job = job_1557079047813_0006\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 12.74 sec   HDFS Read: 2259964 HDFS Write: 1056 SUCCESS\n",
      "Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 6.86 sec   HDFS Read: 5627 HDFS Write: 376 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 19 seconds 600 msec\n",
      "OK\n",
      "14\t1\t0\n",
      "15\t1\t2\n",
      "16\t2\t0\n",
      "17\t0\t1\n",
      "18\t4\t1\n",
      "19\t1\t1\n",
      "20\t0\t2\n",
      "21\t11\t24\n",
      "22\t6\t18\n",
      "23\t12\t15\n",
      "24\t16\t27\n",
      "25\t20\t33\n",
      "26\t23\t44\n",
      "27\t28\t56\n",
      "28\t24\t37\n",
      "29\t24\t66\n",
      "30\t26\t67\n",
      "31\t17\t33\n",
      "32\t13\t48\n",
      "33\t11\t40\n",
      "34\t24\t36\n",
      "35\t12\t42\n",
      "36\t8\t64\n",
      "37\t9\t35\n",
      "38\t6\t17\n",
      "39\t3\t7\n",
      "40\t1\t13\n",
      "41\t5\t20\n",
      "42\t5\t22\n",
      "43\t2\t26\n",
      "44\t7\t35\n",
      "45\t1\t4\n",
      "46\t7\t9\n",
      "47\t1\t1\n",
      "48\t1\t1\n",
      "49\t1\t26\n",
      "50\t1\t26\n",
      "51\t4\t5\n",
      "52\t0\t2\n",
      "53\t0\t2\n",
      "54\t0\t1\n",
      "57\t0\t3\n",
      "58\t1\t57\n",
      "60\t0\t6\n",
      "61\t0\t3\n",
      "64\t2\t1\n",
      "86\t0\t1\n",
      "96\t3\t1\n",
      "Time taken: 81.144 seconds, Fetched: 48 row(s)\n"
     ]
    }
   ],
   "source": [
    "!hive -f task.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
